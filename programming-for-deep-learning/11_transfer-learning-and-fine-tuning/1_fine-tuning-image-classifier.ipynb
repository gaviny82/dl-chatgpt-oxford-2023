{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MhpOfAiFCuGT"
      },
      "source": [
        "# Fine Tuning an Image Classifier in Pytorch\n",
        "\n",
        "In this practical, we will fine-tune a pretrained ResNet-18 model to work with a custom image dataset. The task in this case is to train the model to classify photos depending on which of 10 different global cities they were taken in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c0091643"
      },
      "outputs": [],
      "source": [
        "# @title # Run the following cell to download the necessary files for this practical. { display-mode: \"form\" }\n",
        "# @markdown Don't worry about what's in this collapsed cell\n",
        "\n",
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "# Download dataset.py\n",
        "# [This file has been modified from the original to work on the Windows]\n",
        "# if not os.path.exists('dataset.py'):\n",
        "#     print('Downloading dataset.py...')\n",
        "#     urllib.request.urlretrieve(\n",
        "#         'https://s3-eu-west-1.amazonaws.com/aicore-portal-public-prod-307050600709/practicals_files/41982379-5961-4188-91d7-22fcb7f1c6ef/dataset.py', 'dataset.py')\n",
        "\n",
        "# Download images.zip\n",
        "if not os.path.exists('images.zip') and not os.path.exists('images'):\n",
        "    print('Downloading images.zip...')\n",
        "    urllib.request.urlretrieve(\n",
        "        'https://s3-eu-west-1.amazonaws.com/aicore-portal-public-prod-307050600709/practicals_files/41982379-5961-4188-91d7-22fcb7f1c6ef/images.zip', 'data/images.zip')\n",
        "    \n",
        "    print(\"Extracting iamges...\")\n",
        "    with zipfile.ZipFile('images.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    os.remove('images.zip')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Da_ZorNqCuGX"
      },
      "source": [
        "Run the cell below to import the necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ivVY1g43CuGX"
      },
      "outputs": [],
      "source": [
        "import dataset\n",
        "import torch\n",
        "from dataset import CitiesDataset\n",
        "from torchvision.models import resnet50\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "from torch.optim import lr_scheduler\n",
        "%load_ext tensorboard\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NqJQdOg6CuGY"
      },
      "source": [
        "In the cell below, we will define our network. We create a class that inherits from `torch.nn.Module`, and call the `super().__init__()` method to inherit the methods from the parent class. We have also loaded the `ResNet50` model using the pretrained weights.\n",
        "\n",
        "- Add code to set the `grad_required` argument to False for all the `ResNet` layers.\n",
        "- Define a set of three linear layers for the model, assigning them to the variable `linear_layers`.\n",
        "- The output size of the last layer of `ResNet` is 2048.\n",
        "- The middle layer should have input size 256 and output size of 128.\n",
        "- The output layer should have an output size equivalent to the number of classes in the dataset.\n",
        "\n",
        "- Define the forward method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XjG8NnvaCuGY"
      },
      "outputs": [],
      "source": [
        "class TransferLearning(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = resnet50(weights=ResNet50_Weights)\n",
        "        \n",
        "        # set grad_required = False for all layers of the ResNet50.\n",
        "        for param in self.layers.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # define a three layer network, complete with activation functions between the layers, asssign to a variable called \"linear layers\"\n",
        "        linear_layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(2048, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(256, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 10),\n",
        "        )\n",
        "        self.layers.fc = linear_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-RE2T-qiCuGZ"
      },
      "source": [
        "Next we define a transform, and split the data between train, test and validation sets.\n",
        "\n",
        "- Split the data into train, validation and test sets. The training set should be 70% of dataset length, with the validation and test sets 15% each.\n",
        "- Create a dataloader for each split. Set the batch size to 32, and make sure to set shuffle=True for the training set loader.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ulf-vj80CuGZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jinch\\.conda\\envs\\ML\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "size = 128\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size),\n",
        "    transforms.RandomCrop((size, size), pad_if_needed=True),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = CitiesDataset(transform=transform)\n",
        "model = TransferLearning()\n",
        "\n",
        "# Split the dataset into train, validation and test sets. The train set should be 70% of dataset length, and the validation and test sets 15% each.\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Create dataloaders for train, validation and test sets. Set batch size to 32, and make sure shuffle=True in the train loader.\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "38V_1Gz1CuGa"
      },
      "source": [
        "Prior to training, it will be interesting to see how the classifier performs straight out of the box. In the box below, pass a single example from the test set to the model, with the model in evaluation mode. Get the prediction and compare it to the real label. How did the model do?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O5yDj6qnCuGa",
        "outputId": "f2b852e8-919b-4602-d0df-596e465a731a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 10])\n",
            "Prediction label:  2\n",
            "Prediction category:  Hong Kong, Chian\n",
            "target label: 9\n",
            "target city  Toronto, Canada\n"
          ]
        }
      ],
      "source": [
        "features, label = test_dataset[1]\n",
        "features = features.unsqueeze(0)\n",
        "\n",
        "# Set model to evaluation mode.\n",
        "model.eval()\n",
        "\n",
        "# Pass the features to the model, and assign to a variable called 'outputs'.\n",
        "outputs = model(features)\n",
        "print(outputs.shape)\n",
        "\n",
        "# Get a prediction from the output logits, and assign to a variable called 'prediciton'\n",
        "prediction = torch.argmax(outputs, dim=1)\n",
        "\n",
        "print(\"Prediction label: \", prediction.item())\n",
        "class_label = dataset.idx_to_city_name[prediction.item()]\n",
        "print(\"Prediction category: \", class_label)\n",
        "\n",
        "print(\"target label:\", label)\n",
        "target_label = dataset.idx_to_city_name[label]\n",
        "print(\"target city \", target_label)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bdyMS8zBCuGb"
      },
      "source": [
        "Define the train loop. Some of this has been filled in for you already, but manually fill in the sections corresponding to the `TODO` instructions. This training loop also utilises a learning rate scheduler, which steps the learning rate after a given number of epochs. You don't need to worry about this, but it's good to understand why this can be useful in terms of gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WtC_jaIZCuGb"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    lr=0.1,\n",
        "    epochs=15,\n",
        "    optimiser=torch.optim.SGD\n",
        "):\n",
        " \n",
        "    writer = SummaryWriter()\n",
        "    # initialise an optimiser\n",
        "    optimiser = optimiser(model.parameters(), lr=lr, weight_decay=0.001)\n",
        "    scheduler = lr_scheduler.MultiStepLR(optimiser, milestones=[5,15], gamma=0.1,verbose=True) # learning rate scheduler drops the LR after n epochs, given by \"milestones\"\n",
        "    batch_idx = 0\n",
        "\n",
        "    for epoch in range(epochs):  # for each epoch\n",
        "        \n",
        "        print('Epoch:', epoch,'LR:', scheduler.get_lr())\n",
        "        \n",
        "        for features, labels in train_loader:  # for each batch in the dataloader\n",
        "            # Make a prediction by passing the features to the model\n",
        "            prediction = model(features)\n",
        "\n",
        "            # Calculate the loss \n",
        "            loss = F.cross_entropy(prediction, labels)\n",
        "\n",
        "            # Calculate the gradient of the loss with respect to each model parameter\n",
        "            loss.backward()\n",
        "            \n",
        "            # Use the optimiser to update the model parameters using those gradients\n",
        "            optimiser.step()\n",
        "\n",
        "            print(\"Epoch:\", epoch, \"Batch:\", batch_idx,\n",
        "                  \"Loss:\", loss.item())  # log the loss\n",
        "\n",
        "            # Zero the grad\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
        "            batch_idx += 1\n",
        "                   \n",
        "        scheduler.step() # step the learning rate scheduler\n",
        "\n",
        "        print('Evaluating on valiudation set')\n",
        "        val_loss, val_acc = evaluate(model, val_loader)\n",
        "        writer.add_scalar(\"Loss/Val\", val_loss, batch_idx)\n",
        "        writer.add_scalar(\"Accuracy/Val\", val_acc, batch_idx)\n",
        "    \n",
        "    \n",
        "    print('Evaluating on test set')\n",
        "    test_loss = evaluate(model, test_loader)\n",
        "    model.test_loss = test_loss\n",
        "    \n",
        "    return model   # return trained model\n",
        "    \n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    n_examples = 0\n",
        "    for batch in dataloader:\n",
        "        features, labels = batch\n",
        "        prediction = model(features)\n",
        "        loss = F.cross_entropy(prediction, labels)\n",
        "        losses.append(loss.detach())\n",
        "        correct += torch.sum(torch.argmax(prediction, dim=1) == labels)\n",
        "        n_examples += len(labels)\n",
        "    avg_loss = np.mean(losses)\n",
        "    accuracy = correct / n_examples\n",
        "    print(\"Loss:\", avg_loss, \"Accuracy:\", accuracy.detach().numpy())\n",
        "    return avg_loss, accuracy\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KDHV3bARCuGb"
      },
      "source": [
        "Run the code block below to train the model, and leave it running until it completes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ggvz2H8jCuGb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Epoch: 0 LR: [0.0001]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jinch\\.conda\\envs\\ML\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:429: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 Batch: 0 Loss: 2.3099112510681152\n",
            "Epoch: 0 Batch: 1 Loss: 2.2967610359191895\n",
            "Epoch: 0 Batch: 2 Loss: 2.309739112854004\n",
            "Epoch: 0 Batch: 3 Loss: 2.2913978099823\n",
            "Epoch: 0 Batch: 4 Loss: 2.3150579929351807\n",
            "Epoch: 0 Batch: 5 Loss: 2.3212692737579346\n",
            "Epoch: 0 Batch: 6 Loss: 2.2942349910736084\n",
            "Epoch: 0 Batch: 7 Loss: 2.3242757320404053\n",
            "Epoch: 0 Batch: 8 Loss: 2.277378797531128\n",
            "Epoch: 0 Batch: 9 Loss: 2.2708990573883057\n",
            "Epoch: 0 Batch: 10 Loss: 2.3419251441955566\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.2755375 Accuracy: 0.11594203\n",
            "Epoch: 1 LR: [0.0001]\n",
            "Epoch: 1 Batch: 11 Loss: 2.243016481399536\n",
            "Epoch: 1 Batch: 12 Loss: 2.257474899291992\n",
            "Epoch: 1 Batch: 13 Loss: 2.267336845397949\n",
            "Epoch: 1 Batch: 14 Loss: 2.261023759841919\n",
            "Epoch: 1 Batch: 15 Loss: 2.254497528076172\n",
            "Epoch: 1 Batch: 16 Loss: 2.2692744731903076\n",
            "Epoch: 1 Batch: 17 Loss: 2.265752077102661\n",
            "Epoch: 1 Batch: 18 Loss: 2.256601095199585\n",
            "Epoch: 1 Batch: 19 Loss: 2.2356553077697754\n",
            "Epoch: 1 Batch: 20 Loss: 2.241753101348877\n",
            "Epoch: 1 Batch: 21 Loss: 2.3454089164733887\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.251293 Accuracy: 0.17391305\n",
            "Epoch: 2 LR: [0.0001]\n",
            "Epoch: 2 Batch: 22 Loss: 2.223165988922119\n",
            "Epoch: 2 Batch: 23 Loss: 2.211120843887329\n",
            "Epoch: 2 Batch: 24 Loss: 2.22434663772583\n",
            "Epoch: 2 Batch: 25 Loss: 2.221072196960449\n",
            "Epoch: 2 Batch: 26 Loss: 2.2239909172058105\n",
            "Epoch: 2 Batch: 27 Loss: 2.171328067779541\n",
            "Epoch: 2 Batch: 28 Loss: 2.2004411220550537\n",
            "Epoch: 2 Batch: 29 Loss: 2.213667154312134\n",
            "Epoch: 2 Batch: 30 Loss: 2.2001588344573975\n",
            "Epoch: 2 Batch: 31 Loss: 2.218153715133667\n",
            "Epoch: 2 Batch: 32 Loss: 2.2188191413879395\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.2063413 Accuracy: 0.3043478\n",
            "Epoch: 3 LR: [0.0001]\n",
            "Epoch: 3 Batch: 33 Loss: 2.1916654109954834\n",
            "Epoch: 3 Batch: 34 Loss: 2.172866106033325\n",
            "Epoch: 3 Batch: 35 Loss: 2.1467132568359375\n",
            "Epoch: 3 Batch: 36 Loss: 2.164863348007202\n",
            "Epoch: 3 Batch: 37 Loss: 2.1635429859161377\n",
            "Epoch: 3 Batch: 38 Loss: 2.1575820446014404\n",
            "Epoch: 3 Batch: 39 Loss: 2.112562656402588\n",
            "Epoch: 3 Batch: 40 Loss: 2.1925153732299805\n",
            "Epoch: 3 Batch: 41 Loss: 2.1164944171905518\n",
            "Epoch: 3 Batch: 42 Loss: 2.1627821922302246\n",
            "Epoch: 3 Batch: 43 Loss: 2.236182689666748\n",
            "Adjusting learning rate of group 0 to 1.0000e-04.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.1781967 Accuracy: 0.3043478\n",
            "Epoch: 4 LR: [0.0001]\n",
            "Epoch: 4 Batch: 44 Loss: 2.11407208442688\n",
            "Epoch: 4 Batch: 45 Loss: 2.109658718109131\n",
            "Epoch: 4 Batch: 46 Loss: 2.1240274906158447\n",
            "Epoch: 4 Batch: 47 Loss: 2.1089413166046143\n",
            "Epoch: 4 Batch: 48 Loss: 2.1596529483795166\n",
            "Epoch: 4 Batch: 49 Loss: 2.1026973724365234\n",
            "Epoch: 4 Batch: 50 Loss: 2.112967014312744\n",
            "Epoch: 4 Batch: 51 Loss: 2.0765600204467773\n",
            "Epoch: 4 Batch: 52 Loss: 2.1009750366210938\n",
            "Epoch: 4 Batch: 53 Loss: 2.068072557449341\n",
            "Epoch: 4 Batch: 54 Loss: 2.3468053340911865\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.1610754 Accuracy: 0.2173913\n",
            "Epoch: 5 LR: [1.0000000000000002e-06]\n",
            "Epoch: 5 Batch: 55 Loss: 2.1036264896392822\n",
            "Epoch: 5 Batch: 56 Loss: 2.055135488510132\n",
            "Epoch: 5 Batch: 57 Loss: 2.039393424987793\n",
            "Epoch: 5 Batch: 58 Loss: 2.002511739730835\n",
            "Epoch: 5 Batch: 59 Loss: 2.044280529022217\n",
            "Epoch: 5 Batch: 60 Loss: 2.1189513206481934\n",
            "Epoch: 5 Batch: 61 Loss: 2.0530412197113037\n",
            "Epoch: 5 Batch: 62 Loss: 2.0761420726776123\n",
            "Epoch: 5 Batch: 63 Loss: 2.093991279602051\n",
            "Epoch: 5 Batch: 64 Loss: 2.0273516178131104\n",
            "Epoch: 5 Batch: 65 Loss: 1.9545551538467407\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.1363485 Accuracy: 0.3043478\n",
            "Epoch: 6 LR: [1e-05]\n",
            "Epoch: 6 Batch: 66 Loss: 2.0555942058563232\n",
            "Epoch: 6 Batch: 67 Loss: 2.033463478088379\n",
            "Epoch: 6 Batch: 68 Loss: 2.0444459915161133\n",
            "Epoch: 6 Batch: 69 Loss: 2.0303471088409424\n",
            "Epoch: 6 Batch: 70 Loss: 2.034884214401245\n",
            "Epoch: 6 Batch: 71 Loss: 2.003892660140991\n",
            "Epoch: 6 Batch: 72 Loss: 2.0986545085906982\n",
            "Epoch: 6 Batch: 73 Loss: 2.0376782417297363\n",
            "Epoch: 6 Batch: 74 Loss: 2.025686502456665\n",
            "Epoch: 6 Batch: 75 Loss: 2.0882561206817627\n",
            "Epoch: 6 Batch: 76 Loss: 2.2136876583099365\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.1183522 Accuracy: 0.33333334\n",
            "Epoch: 7 LR: [1e-05]\n",
            "Epoch: 7 Batch: 77 Loss: 2.0617313385009766\n",
            "Epoch: 7 Batch: 78 Loss: 2.0529308319091797\n",
            "Epoch: 7 Batch: 79 Loss: 2.0808591842651367\n",
            "Epoch: 7 Batch: 80 Loss: 2.016291856765747\n",
            "Epoch: 7 Batch: 81 Loss: 2.0543627738952637\n",
            "Epoch: 7 Batch: 82 Loss: 2.0443787574768066\n",
            "Epoch: 7 Batch: 83 Loss: 2.0228874683380127\n",
            "Epoch: 7 Batch: 84 Loss: 2.0566084384918213\n",
            "Epoch: 7 Batch: 85 Loss: 2.0320520401000977\n",
            "Epoch: 7 Batch: 86 Loss: 1.9965429306030273\n",
            "Epoch: 7 Batch: 87 Loss: 2.305661201477051\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.1293917 Accuracy: 0.3478261\n",
            "Epoch: 8 LR: [1e-05]\n",
            "Epoch: 8 Batch: 88 Loss: 2.0001535415649414\n",
            "Epoch: 8 Batch: 89 Loss: 2.037576198577881\n",
            "Epoch: 8 Batch: 90 Loss: 2.0636229515075684\n",
            "Epoch: 8 Batch: 91 Loss: 2.0694613456726074\n",
            "Epoch: 8 Batch: 92 Loss: 2.0083329677581787\n",
            "Epoch: 8 Batch: 93 Loss: 2.0091259479522705\n",
            "Epoch: 8 Batch: 94 Loss: 2.1176869869232178\n",
            "Epoch: 8 Batch: 95 Loss: 2.00395131111145\n",
            "Epoch: 8 Batch: 96 Loss: 2.026250123977661\n",
            "Epoch: 8 Batch: 97 Loss: 2.0171897411346436\n",
            "Epoch: 8 Batch: 98 Loss: 2.2670817375183105\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.114197 Accuracy: 0.36231884\n",
            "Epoch: 9 LR: [1e-05]\n",
            "Epoch: 9 Batch: 99 Loss: 2.0118300914764404\n",
            "Epoch: 9 Batch: 100 Loss: 2.0066072940826416\n",
            "Epoch: 9 Batch: 101 Loss: 1.9774789810180664\n",
            "Epoch: 9 Batch: 102 Loss: 2.098590612411499\n",
            "Epoch: 9 Batch: 103 Loss: 2.0164265632629395\n",
            "Epoch: 9 Batch: 104 Loss: 2.0363645553588867\n",
            "Epoch: 9 Batch: 105 Loss: 2.0725207328796387\n",
            "Epoch: 9 Batch: 106 Loss: 2.0453948974609375\n",
            "Epoch: 9 Batch: 107 Loss: 1.9656789302825928\n",
            "Epoch: 9 Batch: 108 Loss: 2.0359911918640137\n",
            "Epoch: 9 Batch: 109 Loss: 1.7268304824829102\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.1065204 Accuracy: 0.3188406\n",
            "Epoch: 10 LR: [1e-05]\n",
            "Epoch: 10 Batch: 110 Loss: 2.094618558883667\n",
            "Epoch: 10 Batch: 111 Loss: 1.9613772630691528\n",
            "Epoch: 10 Batch: 112 Loss: 2.028761148452759\n",
            "Epoch: 10 Batch: 113 Loss: 1.923459768295288\n",
            "Epoch: 10 Batch: 114 Loss: 2.0000014305114746\n",
            "Epoch: 10 Batch: 115 Loss: 2.022125720977783\n",
            "Epoch: 10 Batch: 116 Loss: 2.038681983947754\n",
            "Epoch: 10 Batch: 117 Loss: 2.0017685890197754\n",
            "Epoch: 10 Batch: 118 Loss: 2.027287483215332\n",
            "Epoch: 10 Batch: 119 Loss: 2.0260119438171387\n",
            "Epoch: 10 Batch: 120 Loss: 2.2185425758361816\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.0939634 Accuracy: 0.3478261\n",
            "Epoch: 11 LR: [1e-05]\n",
            "Epoch: 11 Batch: 121 Loss: 2.0617151260375977\n",
            "Epoch: 11 Batch: 122 Loss: 1.9713209867477417\n",
            "Epoch: 11 Batch: 123 Loss: 2.1007540225982666\n",
            "Epoch: 11 Batch: 124 Loss: 2.0034244060516357\n",
            "Epoch: 11 Batch: 125 Loss: 2.0016496181488037\n",
            "Epoch: 11 Batch: 126 Loss: 1.9661298990249634\n",
            "Epoch: 11 Batch: 127 Loss: 2.046346664428711\n",
            "Epoch: 11 Batch: 128 Loss: 2.030940055847168\n",
            "Epoch: 11 Batch: 129 Loss: 1.9619462490081787\n",
            "Epoch: 11 Batch: 130 Loss: 1.9630175828933716\n",
            "Epoch: 11 Batch: 131 Loss: 1.9952023029327393\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.0964706 Accuracy: 0.36231884\n",
            "Epoch: 12 LR: [1e-05]\n",
            "Epoch: 12 Batch: 132 Loss: 2.0047338008880615\n",
            "Epoch: 12 Batch: 133 Loss: 2.0326828956604004\n",
            "Epoch: 12 Batch: 134 Loss: 2.0828163623809814\n",
            "Epoch: 12 Batch: 135 Loss: 1.8929420709609985\n",
            "Epoch: 12 Batch: 136 Loss: 2.0054330825805664\n",
            "Epoch: 12 Batch: 137 Loss: 1.9810603857040405\n",
            "Epoch: 12 Batch: 138 Loss: 1.9383490085601807\n",
            "Epoch: 12 Batch: 139 Loss: 1.9880688190460205\n",
            "Epoch: 12 Batch: 140 Loss: 1.975877046585083\n",
            "Epoch: 12 Batch: 141 Loss: 2.0565426349639893\n",
            "Epoch: 12 Batch: 142 Loss: 1.924445390701294\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.0689204 Accuracy: 0.39130434\n",
            "Epoch: 13 LR: [1e-05]\n",
            "Epoch: 13 Batch: 143 Loss: 2.0433003902435303\n",
            "Epoch: 13 Batch: 144 Loss: 2.0177671909332275\n",
            "Epoch: 13 Batch: 145 Loss: 2.047642946243286\n",
            "Epoch: 13 Batch: 146 Loss: 1.9501936435699463\n",
            "Epoch: 13 Batch: 147 Loss: 2.1202549934387207\n",
            "Epoch: 13 Batch: 148 Loss: 1.967769980430603\n",
            "Epoch: 13 Batch: 149 Loss: 1.9703553915023804\n",
            "Epoch: 13 Batch: 150 Loss: 1.9834493398666382\n",
            "Epoch: 13 Batch: 151 Loss: 1.913735032081604\n",
            "Epoch: 13 Batch: 152 Loss: 1.9584686756134033\n",
            "Epoch: 13 Batch: 153 Loss: 1.9723360538482666\n",
            "Adjusting learning rate of group 0 to 1.0000e-05.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.094108 Accuracy: 0.3043478\n",
            "Epoch: 14 LR: [1e-05]\n",
            "Epoch: 14 Batch: 154 Loss: 2.013845205307007\n",
            "Epoch: 14 Batch: 155 Loss: 1.9885523319244385\n",
            "Epoch: 14 Batch: 156 Loss: 1.952754259109497\n",
            "Epoch: 14 Batch: 157 Loss: 2.0585975646972656\n",
            "Epoch: 14 Batch: 158 Loss: 1.9814447164535522\n",
            "Epoch: 14 Batch: 159 Loss: 1.936597228050232\n",
            "Epoch: 14 Batch: 160 Loss: 1.969281792640686\n",
            "Epoch: 14 Batch: 161 Loss: 2.0371296405792236\n",
            "Epoch: 14 Batch: 162 Loss: 2.0539169311523438\n",
            "Epoch: 14 Batch: 163 Loss: 1.9609496593475342\n",
            "Epoch: 14 Batch: 164 Loss: 2.2417678833007812\n",
            "Adjusting learning rate of group 0 to 1.0000e-06.\n",
            "Evaluating on valiudation set\n",
            "Loss: 2.0911381 Accuracy: 0.3043478\n",
            "Evaluating on test set\n",
            "Loss: 2.0684364 Accuracy: 0.37142858\n"
          ]
        }
      ],
      "source": [
        "trained_model = train(\n",
        "    model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    test_dataloader,\n",
        "    epochs=15,\n",
        "    lr=0.0001,\n",
        "    optimiser=torch.optim.AdamW\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lxhzjYk6CuGc"
      },
      "source": [
        "Now, let's view training performance in `tensorboard`. Run the cell below to open a `tensorboard` instance, then select `time series` from the dropdown, and press refresh to view the training loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvdJFopICuGc"
      },
      "outputs": [],
      "source": [
        "%tensorboard - -logdir runs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fmGUVAG4CuGc"
      },
      "source": [
        "Finally, let's re-run our prediction code to see how the trained model performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jr31w59DCuGc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction label:  9\n",
            "Prediction category:  Toronto, Canada\n",
            "target label: 9\n",
            "target city  Toronto, Canada\n"
          ]
        }
      ],
      "source": [
        "features, label = test_dataset[1]\n",
        "features = features.unsqueeze(0)\n",
        "model.eval()\n",
        "outputs = model(features)\n",
        "\n",
        "dummy, pred = torch.max(outputs, 1)\n",
        "print(\"Prediction label: \", pred.item())\n",
        "class_label = dataset.idx_to_city_name[pred.item()]\n",
        "print(\"Prediction category: \", class_label)\n",
        "\n",
        "print(\"target label:\", label)\n",
        "target_label = dataset.idx_to_city_name[label]\n",
        "print(\"target city \", target_label)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "f5458f74cc04a2c0765be2b43bd75bd0f09091972ab65a3b08edb45fbf046640"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
