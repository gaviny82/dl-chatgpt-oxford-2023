{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k2LMXYAUA1q"
      },
      "source": [
        "# Training an Image Classifier on the MNIST Dataset\n",
        "\n",
        "Two sentence description of the MNIST dataset.\n",
        "\n",
        "Run the cell below to import the necessary modules and libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QC3JZIImUA1u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A38z6kdUA1v"
      },
      "source": [
        "First, let's create our classifier. \n",
        "- Create a class called `ImageClassifier` that inherits from `torch.nn.Module`.\n",
        "- Make a simple two-layer network inside the class constructor. The input linear layer should have input size appropriate to a 28x28 pixel image, and an output size of 128.\n",
        "- The output linear layer should have an output size of 10, reflecting the number of classes in the `MNIST` dataset.\n",
        "- The two linear layers should be connected by an activation layer.\n",
        "- Don't forget to add inheritance from `nn.Module` by calling the `super` constructor.\n",
        "- Create the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0n0GHBgqUA1v"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define input, activation and output layers\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(28 * 28, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 10),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Softmax()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpoAAiJvUA1v"
      },
      "source": [
        "Next we create our image transform, and load the dataset. We can quickly load the dataset from the `torchvision.datasets` module as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iE0advejUA1v"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "mnist_dataset = datasets.MNIST(root='mnist', train=True, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGb9Tgg5UA1w"
      },
      "source": [
        "Now we need to perform a split on the data so that we can train our model and evaluate it. \n",
        "\n",
        "- Split the dataset into a training set comprising 80% of the data, and a test set comprising 20% of the data. Call these subsets `train_set` and `test_set`.\n",
        "- Assign each split to its own dataloader, called `train_loader` and `test_loader` respectively. Set `shuffle=True` for the train loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yIgN0elpUA1x"
      },
      "outputs": [],
      "source": [
        "# Define the sizes of the train and test set\n",
        "total_size = len(mnist_dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "test_size = total_size - train_size\n",
        "\n",
        "# Use the `random_split()` method to create a `train` and `test` dataset\n",
        "train_data, test_data = random_split(mnist_dataset, [train_size, test_size])\n",
        "\n",
        "# Create the training dataloader\n",
        "train_loader = data.DataLoader(train_data, shuffle=True, pin_memory=True)\n",
        "\n",
        "# Create the test dataloader\n",
        "train_loader = data.DataLoader(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hftLokuoUA1x"
      },
      "source": [
        "To get everything ready for training, we need to initialise the model, an optimiser and a criterion. In the code block below, initialise an instance of your model class, as well as an optimiser for Stochastic Gradient Descent (SGD), and an appropriate loss criterion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nwcuCdNXUA1y"
      },
      "outputs": [],
      "source": [
        "# Initialize the model and optimizer\n",
        "\n",
        "model = SimpleClassifier()\n",
        "optimiser = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.functional.cross_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSG3eaRoUA1y"
      },
      "source": [
        "Create the training loop inside a function called `train`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wlUHEeyMUA1y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jinch\\.conda\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.8761663240393003\n",
            "Epoch [2/10], Loss: 1.689560981084903\n",
            "Epoch [3/10], Loss: 1.656089256733656\n",
            "Epoch [4/10], Loss: 1.6406358544031778\n",
            "Epoch [5/10], Loss: 1.6157760947346687\n",
            "Epoch [6/10], Loss: 1.5386700637340545\n",
            "Epoch [7/10], Loss: 1.5301534321208794\n",
            "Epoch [8/10], Loss: 1.522450298746427\n",
            "Epoch [9/10], Loss: 1.5194228985011577\n",
            "Epoch [10/10], Loss: 1.513732946028312\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "\n",
        "def train(model: torch.nn.Module, train_loader: data.DataLoader, optimiser: optim.Optimizer, criterion) -> None:\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            model.zero_grad()\n",
        "            predictions = model(images)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            optimiser.step()\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/10], Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "train(model, train_loader, optimiser, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89OeHfzXUA1z"
      },
      "source": [
        "Now let's see how the model performs on an example from the testing set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PD4W0zAlUA1z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Predicted label: 1\n",
            "Label: 1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jinch\\AppData\\Local\\Temp\\ipykernel_26036\\241311094.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  probabilities: numpy.ndarray = torch.nn.functional.softmax(prediction).detach().numpy()\n"
          ]
        }
      ],
      "source": [
        "test_example, test_label = next(iter(test_data))\n",
        "model.eval()\n",
        "\n",
        "prediction = model(test_example)\n",
        "probabilities: numpy.ndarray = torch.nn.functional.softmax(prediction).detach().numpy()\n",
        "\n",
        "predicted_label = probabilities.argmax()\n",
        "print(f\"\"\"\n",
        "Predicted label: {predicted_label}\n",
        "Label: {test_label}\n",
        "\"\"\")\n",
        "\n",
        "# TODO - Get a single example from the test dataset\n",
        "# TODO - Set model to eval()\n",
        "# TODO - Pass the example to the model to get the prediction logits.\n",
        "# TODO - Take the softmax of the logits\n",
        "# TODO - Print the class label for the prediction of highest likelihood.\n",
        "# TODO - Print the real target label for the example."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\jinch\\.conda\\envs\\ML\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\jinch\\Documents\\Programming\\dl-chatgpt-oxford-2023\\programming-for-deep-learning\\7_understanding-neural-networks\\1_MINST.ipynb Cell 14\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimiser \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m train_gpu(model, train_loader, optimiser, criterion)\n",
            "\u001b[1;32mc:\\Users\\jinch\\Documents\\Programming\\dl-chatgpt-oxford-2023\\programming-for-deep-learning\\7_understanding-neural-networks\\1_MINST.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_gpu\u001b[1;34m(model, train_loader, optimiser, criterion)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/7_understanding-neural-networks/1_MINST.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimiser\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\jinch\\.conda\\envs\\ML\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[1;32mc:\\Users\\jinch\\.conda\\envs\\ML\\lib\\site-packages\\torch\\autograd\\__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    163\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    165\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 166\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
            "File \u001b[1;32mc:\\Users\\jinch\\.conda\\envs\\ML\\lib\\site-packages\\torch\\autograd\\__init__.py:68\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     67\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format))\n\u001b[0;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train_gpu(model: torch.nn.Module, train_loader: data.DataLoader, optimiser: optim.Optimizer, criterion) -> None:\n",
        "    for epoch in range(10):\n",
        "        print(f\"Epoch [{epoch + 1}/10], \", end='')\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            torch.cuda.synchronize()\n",
        "            images = images.to(\"cuda\")\n",
        "            labels = labels.to(\"cuda\")\n",
        "\n",
        "            model.zero_grad()\n",
        "            predictions = model(images)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "            loss.backward()\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            optimiser.step()\n",
        "\n",
        "        print(f'Loss: {running_loss / len(train_loader)}')\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "model = SimpleClassifier().to(\"cuda\")\n",
        "optimiser = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = nn.functional.cross_entropy\n",
        "train_gpu(model, train_loader, optimiser, criterion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f5458f74cc04a2c0765be2b43bd75bd0f09091972ab65a3b08edb45fbf046640"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
