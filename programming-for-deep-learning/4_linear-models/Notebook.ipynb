{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EbGuBe6rEcsm"
      },
      "source": [
        "# Linear Models\n",
        "\n",
        "> Linear models are those which use a linear function to combine features into a prediction\n",
        "\n",
        "When your data only has a single scalar feature (e.g. height) and a single scalar target (e.g. age), that looks like a straight line equation:\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/2.%20Pytorch/2.%20Linear%20Models/images/Linear%20Model%20Graph%20and%20Equation.png?raw=1)\n",
        "\n",
        "> When a linear model is used to make a prediction that can be any continuous number, the model is known as _linear regression_.\n",
        "\n",
        "- Regression: Output can be any continuous value\n",
        "- Linear: Output is a weighted sum of inputs, plus some constant (bias)\n",
        "\n",
        "Of course, in many problems of interest, the data can have many features. This makes it harder to visualise, but the concept is exactly the same.\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/2.%20Pytorch/2.%20Linear%20Models/images/Linear%20Regression%20Equation.png?raw=1)\n",
        "\n",
        "This means that each feature has an associated _weight_ which represents its influence on the prediction.\n",
        "\n",
        "The prediction made by linear regression changes by a constant value as the value of each feature increases by one. That constant value, is the associated feature's weight.\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/2.%20Pytorch/2.%20Linear%20Models/images/Linear%20Regression%20Parameter%20Example.png?raw=1)\n",
        "\n",
        "Linear regression model parameters:\n",
        "\n",
        "- Weight: Influence of each feature\n",
        "- Bias: A constant value for each output\n",
        "\n",
        "> Note that, although the bias of a linear layer is often a scalar, a linear layer has as many biases as it has outputs\n",
        "\n",
        "The linear layer can be represented graphically too:\n",
        "\n",
        "![](https://github.com/AI-Core/Content-Public/blob/main/Content/units/Towards%20ChatGPT/2.%20Pytorch/2.%20Linear%20Models/images/Linear%20Layer%20Graphical%20Model%20with%20Labels.png?raw=1)\n",
        "\n",
        "In PyTorch, the _`Linear` layer_ can be used to create a (initially random) linear model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WHFmxBO6Ecsp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "linear_layer = torch.nn.Linear(3, 1)  # 3 input features, 1 output prediction\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "udOrf4hDEcsq"
      },
      "source": [
        "PyTorch's `Linear` layers contain the model parameters - both the weights and the bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I3iy-5XbEcsq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "weight: Parameter containing:\n",
            "tensor([[ 0.5183,  0.5672, -0.1289]], requires_grad=True)\n",
            "bias: Parameter containing:\n",
            "tensor([-0.5486], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(\"weight:\", linear_layer.weight)\n",
        "print(\"bias:\", linear_layer.bias)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S3pjyinxEcsq"
      },
      "source": [
        "We can use this linear layer in a PyTorch model to create a linear regression model (which is initialised with random parameters):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0LYsFfa4Ecsr"
      },
      "outputs": [],
      "source": [
        "class LinearRegression(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        self.linear_layer = torch.nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.linear_layer(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRY_ybJEEcsr"
      },
      "source": [
        "> Linear models are differentiable, which means that they can be trained from end to end using gradient descent\n",
        "\n",
        "This can be done in a typical PyTorch training loop:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IQNv13WvEcss"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "def train(model, dataloader, epochs=10):\n",
        "\n",
        "    # SET UP LOGGING\n",
        "    writer = SummaryWriter()\n",
        "    batch_idx = 0\n",
        "\n",
        "    optimiser = torch.optim.SGD(\n",
        "        model.parameters(), lr=0.001)  # SET UP OPTIMISER\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataloader:\n",
        "\n",
        "            features, labels = batch  # UNPACK BATCH OF DATA\n",
        "\n",
        "            # COMPUTE LOSS\n",
        "            predictions = model(features)\n",
        "            loss = F.mse_loss(predictions, labels)\n",
        "\n",
        "            # OPTIMISE\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            # LOGGING\n",
        "            writer.add_scalar(\"Loss/Train\", loss.item(), batch_idx)\n",
        "            batch_idx += 1\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I-zCi5PwEcss"
      },
      "source": [
        "Usually, you'd start with the data. But in our case we don't have it yet, so let's get some, and then use PyTorch's `DataLoader` to shuffle it and batch examples together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "piIA3DcDEcss"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "random_split() missing 1 required positional argument: 'lengths'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\jinch\\Documents\\Programming\\dl-chatgpt-oxford-2023\\programming-for-deep-learning\\4_linear-models\\Notebook.ipynb Cell 10\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/4_linear-models/Notebook.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/4_linear-models/Notebook.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn((\u001b[39m100\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/4_linear-models/Notebook.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_set, val_set, test_set \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mrandom_split(dataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jinch/Documents/Programming/dl-chatgpt-oxford-2023/programming-for-deep-learning/4_linear-models/Notebook.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_set, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: random_split() missing 1 required positional argument: 'lengths'"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = 0  # See practice 1\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset)\n",
        "\n",
        "train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VFgNd27yEcst"
      },
      "source": [
        "Now we have everything we need to train our linear regression model, so let's train it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AYYMG4wEcst"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "train(model, train_loader)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TUsmy3KsEcst"
      },
      "source": [
        "Once trained, we can check out the values of the linear layer's parameters to see where they ended up:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqyLZcCwEcst"
      },
      "outputs": [],
      "source": [
        "print(\"Final weight:\", model.linear_layer.weight)\n",
        "print(\"Final bias:\", model.linear_layer.bias)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f5458f74cc04a2c0765be2b43bd75bd0f09091972ab65a3b08edb45fbf046640"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
